{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndataset = pd.read_csv(\"bengaluru.csv\")\\ndataset.head(3)\\ndataset.shape\\ndataset.describe()\\ndataset.info()\\n# sns.pairplot(data=dataset)\\n# plt.show()\\ndataset = dataset.drop([\\'totalSnow_cm\\', \\'date_time\\'], axis=1)  # [\\'income\\', \\'expenses\\']\\ndataset.drop_duplicates(inplace=True)\\ndataset.isnull().sum()\\nobject_data = dataset.select_dtypes(include=[\\'object\\'])\\nobject_data\\ndataset[\"moonrise\"].value_counts()\\nfrom sklearn.preprocessing import LabelEncoder\\n\\nle = LabelEncoder()\\n    \\nfor column in dataset.select_dtypes(include=[\\'object\\']).columns:\\n    dataset[column] = le.fit_transform(dataset[column])\\n# if want to convert int32 to int64\\n\\n# dataset = dataset.astype({col: \\'int64\\' for col in dataset.select_dtypes(include=\\'int32\\').columns})\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\n\\nnumerical_columns = dataset.select_dtypes(include=[\\'float64\\', \\'int64\\', \\'int32\\']).columns\\n\\ndataset[numerical_columns] = scaler.fit_transform(dataset[numerical_columns])\\ndataset.head(5)\\nQ1 = dataset.quantile(0.25)\\nQ3 = dataset.quantile(0.75)\\n\\nIQR = Q3 - Q1\\n\\nlower_bound = Q1 - 1.5 * IQR\\nupper_bound = Q3 + 1.5 * IQR\\n\\ndataset = dataset[~((dataset < lower_bound) | (dataset > upper_bound)).any(axis=1)]\\ndataset.shape\\n\\n# (96432, 25) previus shape of dataset\\n#finding outliers\\nfig, ax = plt.subplots(figsize=(20, 8))\\ndataset.boxplot(ax=ax)\\n\\n# Customize the plot\\nax.set_title(\\'Boxplots of Variables\\')\\nax.set_xlabel(\\'Variables\\')\\nax.set_ylabel(\\'Values\\')\\nplt.show()\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\n\\na = dataset.drop(columns=[\\'maxtempC\\'])  # Drop the target column\\nb = dataset[\\'maxtempC\\']\\n\\na_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.3, random_state=42)\\n\\nmodel = LinearRegression()\\nfrom mlxtend.feature_selection import SequentialFeatureSelector\\n\\nfs = SequentialFeatureSelector(model, k_features=8, forward=True,  scoring=\\'r2\\',  cv=5)          \\n\\n\\nfs.fit(a_train, b_train)\\n\\n# Get the selected features and their indices\\nselected_features = fs.k_feature_names_\\nprint(f\"Selected features: {selected_features}\")\\n\\nselected_indices = fs.k_feature_idx_\\nprint(f\"Selected feature indices: {selected_indices}\")\\n\\nfrom sklearn.feature_selection import RFE\\n\\nrfe = RFE(estimator=model, n_features_to_select=9)\\n\\nrfe.fit(a_train, b_train)\\n\\nselected_features = a_train.columns[rfe.support_]\\n\\nselected_features\\n\\ncorrelation_matrix = dataset.corr()\\n#  Visualize the correlation matrix using a heatmap\\nplt.figure(figsize=(25,10))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\\'coolwarm\\', linewidths=0.5)\\nplt.title(\\'Correlation Matrix\\')\\nplt.show()\\ny_split = pd.DataFrame(dataset[[\\'maxtempC\\', \\'windspeedKmph\\']])\\ny_split\\n# x = dataset.drop(columns=[\\'maxtempC\\', \\'windspeedKmph\\'])\\n# y = dataset[[\\'maxtempC\\', \\'windspeedKmph\\']]\\n\\n# x = dataset.drop(columns=[\\'maxtempC\\', \\'windspeedKmph\\'])\\n# y = y_split\\n\\nx = dataset.drop(columns=[\\'maxtempC\\'])\\ny = dataset[\\'maxtempC\\']\\nfrom sklearn.model_selection import train_test_split\\n\\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=7)\\ny_train.shape\\nfrom itertools import combinations\\n\\nfrom sklearn.model_selection import cross_val_score\\n\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.neural_network import MLPRegressor\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.svm import SVR\\nmodels = {\\n    \\'Linear\\': LinearRegression(),\\n    \\'MLP\\': MLPRegressor(max_iter=500), \\n    \\'KNeighbors\\': KNeighborsRegressor(),\\n    \\'DecisionTree\\': DecisionTreeRegressor(),\\n    \\'RandomForest\\': RandomForestRegressor()\\n}\\n\\n# Define the models\\nlr = LinearRegression()\\nknr = KNeighborsRegressor()\\nmlp = MLPRegressor(random_state=42, max_iter=1000)  \\ndt = DecisionTreeRegressor(random_state=42)\\nrfc = RandomForestRegressor(random_state=42)\\nsv = SVR()\\n\\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, max_error\\n\\ndef matrix_error(y_true, y_pred, n_features):\\n    mae = mean_absolute_error(y_true, y_pred)*100\\n    mse = mean_squared_error(y_true, y_pred)*100\\n    rmse = np.sqrt(mse) \\n    r2 = r2_score(y_true, y_pred)*100\\n    \\n    max_err = max_error(y_true, y_pred)    \\n    \\n    # Huber Loss\\n    delta = 1.0  # You can adjust delta based on your needs\\n    huber_loss = np.mean(np.where(np.abs(y_true - y_pred) <= delta, \\n                                  0.5 * (y_true - y_pred)**2, \\n                                  delta * (np.abs(y_true - y_pred) - 0.5 * delta)))*100\\n    \\n    # Relative Squared Error (RSE)\\n    rse = np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2)*100\\n    \\n    # Relative Absolute Error (RAE)\\n    rae = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true - np.mean(y_true)))*100\\n\\n    # Adjusted R-squared\\n    if x_train is not None:\\n        n = len(y_true)\\n        p = x_train.shape[1]\\n        adj_r2 = (1 - (1 - r2 / 100) * ((n - 1) / (n - p - 1)))  # Convert Adjusted RÂ² to percentage\\n    else:\\n        adj_r2 = 0\\n\\n\\n    return {\\n        \\'RÂ²\\': r2,\\n        \\'MAE\\': mae,\\n        \\'MSE\\': mse,\\n        \\'RMSE\\': rmse,\\n        \\'Adj RÂ²\\': adj_r2,\\n        \\'Max Error\\': max_err,\\n        \\'Huber Loss\\': huber_loss,\\n        \\'RSE\\': rse,\\n        \\'RAE\\': rae\\n    }\\n\\n# Loop through each model\\nfor name, model in models.items():\\n    \\n    model.fit(x_train, y_train)\\n      \\n    train_predictions = model.predict(x_train)\\n    test_predictions = model.predict(x_test)\\n\\n    train_metrics = matrix_error(y_train, train_predictions, x_train.shape[1])\\n    test_metrics = matrix_error(y_test, test_predictions, x_train.shape[1])  \\n\\n    # Print the metrics\\n    print(f\"{name} - Train Metrics: {train_metrics}\")\\n    print(f\"{name} - Test Metrics: {test_metrics}\\\\n\")\\nfrom sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, StackingRegressor, VotingRegressor\\n\\n# Function to fit models, average predictions, and evaluate all error metrics\\ndef fit_and_evaluate_combination(models_to_fit, x_train, x_test, y_train, y_test, ensemble_type=\\'average\\'):\\n     \\n    #  ensemble_type == \\'average\\':\\n    #     train_predictions = np.zeros(len(y_train))\\n    #     test_predictions = np.zeros(len(y_test))\\n\\n    #     for model_name in models_to_fit:\\n    #         model = models[model_name]\\n    #         model.fit(x_train, y_train)\\n    #         train_predictions += model.predict(x_train)\\n    #         test_predictions += model.predict(x_test)\\n\\n    #     train_predictions /= len(models_to_fit)\\n    #     test_predictions /= len(models_to_fit)\\n\\n    if ensemble_type == \\'bagging\\':\\n        # Bagging with the first model as base\\n        base_model = models[models_to_fit[0]]\\n        bagging_model = BaggingRegressor(estimator=base_model, n_estimators=10)\\n        bagging_model.fit(x_train, y_train)\\n        train_predictions = bagging_model.predict(x_train)\\n        test_predictions = bagging_model.predict(x_test)\\n\\n    elif ensemble_type == \\'boosting\\':\\n        # Boosting with a specified base model\\n        base_model = models[models_to_fit[0]]\\n        boosting_model = GradientBoostingRegressor(n_estimators=100)\\n        boosting_model.fit(x_train, y_train)\\n        train_predictions = boosting_model.predict(x_train)\\n        test_predictions = boosting_model.predict(x_test)\\n\\n    elif ensemble_type == \\'stacking\\':\\n        # Stacking Regressor with selected models\\n        base_models = [models[model_name] for model_name in models_to_fit]\\n        stacking_model = StackingRegressor(estimators=[(name, model) for name, model in zip(models_to_fit, base_models)], final_estimator=LinearRegression())\\n        stacking_model.fit(x_train, y_train)\\n        train_predictions = stacking_model.predict(x_train)\\n        test_predictions = stacking_model.predict(x_test)\\n\\n    elif ensemble_type == \\'voting\\':\\n        # Voting Regressor with selected models\\n        voting_model = VotingRegressor(estimators=[(name, models[name]) for name in models_to_fit])\\n        voting_model.fit(x_train, y_train)\\n        train_predictions = voting_model.predict(x_train)\\n        test_predictions = voting_model.predict(x_test)\\n\\n    # Calculate error metrics for train and test sets\\n    train_metrics = matrix_error(y_train, train_predictions, x_train.shape[1])\\n    test_metrics = matrix_error(y_test, test_predictions, x_test.shape[1])\\n\\n    return train_metrics, test_metrics\\n# Loop through all combinations of 2 to 5 models and print error metrics\\nfor num_models in range(2, 3):  \\n    for model_combination in combinations(models.keys(), num_models): \\n        \\n\\n# # Loop through all combinations of 3 models, starting from the 6th combination\\n# model_combinations = list(combinations(models.keys(), 3))  # Get all combinations of 3 models\\n# for i, model_combination in enumerate(model_combinations):\\n#     if i < 5:  # Skip the first 6 combinations\\n#         continue\\n\\n        # Average Ensemble\\n        # train_metrics, test_metrics = fit_and_evaluate_combination(model_combination, x_train, x_test, y_train, y_test, ensemble_type=\\'average\\')\\n        # print(f\"Average - Train Metrics: {train_metrics}\")\\n        # print(f\"Average - Test Metrics: {test_metrics}\")\\n        \\n    # Bagging Ensemble\\n        train_metrics, test_metrics = fit_and_evaluate_combination(model_combination, x_train, x_test, y_train, y_test, ensemble_type=\\'bagging\\')\\n        print(f\"Bagging Combination: {model_combination} - Train Metrics: {train_metrics}\")\\n        print(f\"Bagging Combination: {model_combination} - Test Metrics: {test_metrics}\\\\n\")\\n\\n        # Boosting Ensemble\\n        train_metrics, test_metrics = fit_and_evaluate_combination(model_combination, x_train, x_test, y_train, y_test, ensemble_type=\\'boosting\\')\\n        print(f\"Boosting Combination: {model_combination} - Train Metrics: {train_metrics}\")\\n        print(f\"Boosting Combination: {model_combination} - Test Metrics: {test_metrics}\\\\n\")\\n\\n        # Stacking Ensemble\\n        train_metrics, test_metrics = fit_and_evaluate_combination(model_combination, x_train, x_test, y_train, y_test, ensemble_type=\\'stacking\\')\\n        print(f\"Stacking Combination: {model_combination} - Train Metrics: {train_metrics}\")\\n        print(f\"Stacking Combination: {model_combination} - Test Metrics: {test_metrics}\\\\n\")\\n\\n        # Voting Ensemble\\n        train_metrics, test_metrics = fit_and_evaluate_combination(model_combination, x_train, x_test, y_train, y_test, ensemble_type=\\'voting\\')\\n        print(f\"Voting Combination: {model_combination} - Train Metrics: {train_metrics}\")\\n        print(f\"Voting Combination: {model_combination} - Test Metrics: {test_metrics}\\\\n\")\\n\\n        print(\"-\" * 40)\\n# \"\"\"GridSearchCV\"\"\"\\n\\n# from sklearn.model_selection import GridSearchCV\\n\\n# param_grid = {\\n#     \\'alpha\\': [0.1, 1.0, 10.0, 100.0, 1000.0],  # Regularization strength\\n#     \\'solver\\': [\\'auto\\', \\'svd\\', \\'cholesky\\', \\'lsqr\\', \\'sag\\'],  # Solver options\\n#     \\'max_iter\\': [100, 200, 500],  # Maximum number of iterations\\n#     \\'tol\\': [1e-4, 1e-3, 1e-2],  # Tolerance for stopping criteria\\n#     \\'fit_intercept\\': [True, False]  # Whether to calculate the intercept\\n# }\\n\\n# grid_search = GridSearchCV(estimator= lr, param_grid= param_grid, cv=2, verbose=1, n_jobs=-1)\\n\\n# grid_search.fit(x_train, y_train)\\n\\n\\n# best_params = grid_search.best_params_\\n# best_params\\nnew_data = pd.DataFrame({\\n    \\'mintempC\\': [-0.767782],       \\n    \\'sunHour\\': [-1.202966],\\n    \\'uvIndex\\': [0.476419],\\n    \\'uvIndex.1\\': [-1.057681],\\n    \\'moon_illumination\\': [0.343652],\\n    \\'moonrise\\': [-0.259014],\\n    \\'moonset\\': [0.925991],\\n    \\'sunrise\\': [1.01614],\\n    \\'sunset\\' : [-0.968057],\\n    \\'DewPointC\\' : [-0.020525],\\n    \\'FeelsLikeC\\' : [-1.526577],\\n    \\'HeatIndexC\\' : [-1.640715],\\n    \\'WindChillC\\': [-1.421855],\\n    \\'WindGustKmph\\': [-1.531160],\\n    \\'cloudcover\\': [0.040594],\\n    \\'humidity\\': [0.919832],\\n    \\'precipMM\\': [-0.201416],\\n    \\'pressure\\': [1.708747],\\n    \\'tempC\\': [-1.317137],\\n    \\'visibility\\': [0.332235],\\n    \\'winddirDegree\\': [-0.672856],\\n    \\'windspeedKmph\\': [-1.303005]  # Uncomment if you want to include this feature\\n})\\n\\nfor i, model in enumerate(models):\\n    \\n    predicted_max_temp = model.predict(new_data)\\n    \\n    predicted_value_max_temp = predicted_max_temp[0] if predicted_max_temp.ndim == 1 else predicted_max_temp[0][0]\\n    \\n    print(f\"{model_names[i]} - Predicted Maximum Temperature: {predicted_value_max_temp:.2f} Â°C\")\\n    \\n    # # If there is more than one predicted value (e.g., for windspeed), handle accordingly\\n    # if predicted_max_temp.ndim > 1 and predicted_max_temp.shape[1] > 1:\\n    #     predicted_value_wind_speed = predicted_max_temp[0][1]\\n    #     print(f\"{model_names[i]} - Predicted Wind Speed: {predicted_value_wind_speed:.2f} Kmph\")\\n    \\n    print()  \\n    \\n    \\ndataset.head(4)\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = r\"C:\\Users\\DELL\\Desktop\\diff project\\weather regression\\weather pro\\weather_short-cut11.ipynb\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    notebook_content = json.load(file)\n",
    "  \n",
    "# Extracting code cells\n",
    "code_cells = [\n",
    "    cell['source']\n",
    "    for cell in notebook_content.get('cells', [])\n",
    "    if cell.get('cell_type') == 'code'\n",
    "]\n",
    "\n",
    "# Combine all code cells into a single script for analysis\n",
    "code_combined = \"\\n\".join([\"\".join(cell) for cell in code_cells])\n",
    "code_combined[:20000]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
